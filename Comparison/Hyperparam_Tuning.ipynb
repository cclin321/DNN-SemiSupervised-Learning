{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pandas numpy scikit-learn matplotlib shap openpyxl scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3add1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, tensorflow as tf\n",
    "print(sys.executable)  \n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "\n",
    "random.seed(42); np.random.seed(42); tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88caa28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TARGET = \"Grid  Power\"   \n",
    "FILTER_Y_EQ_ZERO = True \n",
    "K_NEIGHBORS = 1       \n",
    "PAIR_SAMPLE_FRAC = 0.10  \n",
    "PAIR_SAMPLE_MAX = 200_000\n",
    "EPOCHS_STAGE2 = 100\n",
    "EPOCHS_STAGE3 = 100\n",
    "\n",
    "\n",
    "TUNING_TRIALS = 20  \n",
    "CV_SPLITS     = 3   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_data = pd.read_excel('raw data.xlsx')\n",
    "\n",
    "raw_data = raw_data.drop(columns=['times'], errors='ignore').copy()\n",
    "\n",
    "\n",
    "if FILTER_Y_EQ_ZERO:\n",
    "    raw_data = raw_data[raw_data[TARGET] != 0].copy()\n",
    "\n",
    "\n",
    "num_cols = [c for c in raw_data.columns if c != TARGET]\n",
    "raw_data[num_cols] = raw_data[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "raw_data = raw_data.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Data shape:\", raw_data.shape)\n",
    "display(raw_data.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = raw_data.sample(frac=0.8, random_state=0)\n",
    "test_dataset  = raw_data.drop(train_dataset.index)\n",
    "\n",
    "test_x_data = test_dataset.drop(columns=[TARGET]).copy()\n",
    "test_y_data = test_dataset[TARGET].copy()\n",
    "\n",
    "\n",
    "labeled_train_data   = train_dataset.sample(frac=0.6, random_state=0).copy()\n",
    "unlabeled_train_data = train_dataset.drop(labeled_train_data.index).copy()\n",
    "\n",
    "\n",
    "unlabeled_train_data_actual = unlabeled_train_data.pop(TARGET).copy()\n",
    "labeled_data_labels         = labeled_train_data.pop(TARGET).copy()\n",
    "\n",
    "len(train_dataset), len(labeled_train_data), len(unlabeled_train_data), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_stats = train_dataset.describe().transpose()\n",
    "train_stats = train_stats.drop(index=TARGET, errors='ignore')\n",
    "\n",
    "def norm(df):\n",
    "    return (df - train_stats['mean']) / train_stats['std'].replace(0, 1.0)\n",
    "\n",
    "normed_labeled_train_data   = norm(labeled_train_data).astype('float32')\n",
    "normed_unlabeled_train_data = norm(unlabeled_train_data).astype('float32')\n",
    "normed_test_data            = norm(test_x_data).astype('float32')\n",
    "\n",
    "INPUT_DIM = normed_labeled_train_data.shape[1]\n",
    "print(\"Input dim:\", INPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric=\"euclidean\").fit(normed_labeled_train_data.values)\n",
    "dists, idxs = nbrs.kneighbors(normed_unlabeled_train_data.values, return_distance=True)\n",
    "\n",
    "\n",
    "initial_labels = labeled_data_labels.iloc[idxs.ravel()].reset_index(drop=True)\n",
    "unlabeled_train_data_actual = unlabeled_train_data_actual.reset_index(drop=True)\n",
    "\n",
    "pd.DataFrame(initial_labels, columns=[TARGET]).to_excel('outputs/20230315_initial_labels.xlsx', index=False)\n",
    "pd.DataFrame(unlabeled_train_data_actual, columns=[TARGET]).to_excel('outputs/20230315_actual_labels.xlsx', index=False)\n",
    "\n",
    "print(\"Stage-1 finished：initial tag num\", len(initial_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6386b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZL = normed_labeled_train_data.values.astype('float32')  # (m, p)\n",
    "yL = labeled_data_labels.values.astype('float32')\n",
    "m, p = ZL.shape\n",
    "\n",
    "\n",
    "PAIR_SAMPLE_FRAC = 0.10\n",
    "PAIR_SAMPLE_MAX  = 200_000\n",
    "BATCH_GEN        = 50_000 \n",
    "\n",
    "total_pairs = m * (m - 1) // 2\n",
    "n_sample = min(max(1, int(PAIR_SAMPLE_FRAC * total_pairs)), PAIR_SAMPLE_MAX)\n",
    "\n",
    "rng = np.random.default_rng(9)\n",
    "Xs, ys, made = [], [], 0\n",
    "while made < n_sample:\n",
    "    b = min(BATCH_GEN, n_sample - made)\n",
    "    i = rng.integers(0, m-1, size=b, endpoint=False)\n",
    "    j = rng.integers(0, m,   size=b)\n",
    "    mask = i < j\n",
    "    if not np.any(mask):\n",
    "        continue\n",
    "    i = i[mask]; j = j[mask]\n",
    "\n",
    "    Xs.append(ZL[i] - ZL[j])         # (b, p), float32\n",
    "    ys.append(yL[i] - yL[j])         # (b,)\n",
    "    made += len(i)\n",
    "\n",
    "s_feature_diff_labeled = pd.DataFrame(np.vstack(Xs).astype('float32'))\n",
    "s_target_diff          = pd.Series(np.concatenate(ys), dtype='float32')\n",
    "print(\"Stage-2 number of sampled differences:\", len(s_target_diff))\n",
    "print(\"Stage-2 number of sampled differences:\", s_feature_diff_labeled.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d693d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def build_model(input_dim):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mae\",\"mse\"])\n",
    "    return model\n",
    "'''\n",
    "\n",
    "# Tuning\n",
    "def build_model(input_dim,\n",
    "                layers_: int = 3,\n",
    "                units: int = 32,\n",
    "                dropout: float = 0.0,\n",
    "                l2: float = 0.0,\n",
    "                activation: str = \"relu\",\n",
    "                learning_rate: float = 1e-3):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    kreg = regularizers.l2(l2) if (l2 and l2 > 0) else None\n",
    "    for _ in range(layers_):\n",
    "        x = layers.Dense(units, kernel_regularizer=kreg)(x)\n",
    "        if activation == \"leaky_relu\":\n",
    "            x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            x = layers.Activation(activation)(x)\n",
    "        if dropout and dropout > 0:\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=\"mae\", metrics=[\"mae\",\"mse\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "modification_model = build_model(INPUT_DIM)\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 25 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "history = modification_model.fit(\n",
    "    s_feature_diff_labeled, s_target_diff,\n",
    "    epochs=EPOCHS_STAGE2, validation_split=0.2, verbose=0, callbacks=[PrintDot()]\n",
    ")\n",
    "\n",
    "hist2 = pd.DataFrame(history.history); hist2['epoch'] = history.epoch\n",
    "pd.DataFrame(hist2).to_excel('outputs/stage2_errors.xlsx', index=False)\n",
    "hist2.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modification_model.summary()\n",
    "print(\"X shape:\", s_feature_diff_labeled.shape, \"y shape:\", s_target_diff.shape, s_target_diff.dtype)\n",
    "\n",
    "\n",
    "eval_loss, eval_mae, eval_mse = modification_model.evaluate(s_feature_diff_labeled, s_target_diff, verbose=0)\n",
    "print(\"Eval -> loss/mae/mse:\", eval_loss, eval_mae, eval_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d265ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "closestL_idx = idxs.ravel()\n",
    "\n",
    "\n",
    "ZL_df = pd.DataFrame(ZL)\n",
    "ZU_df = normed_unlabeled_train_data.reset_index(drop=True)\n",
    "feature_diff_most_sim = (ZL_df.iloc[closestL_idx].values - ZU_df.values).astype('float32')\n",
    "\n",
    "\n",
    "modification_values = modification_model.predict(feature_diff_most_sim, verbose=0).ravel().astype('float32')\n",
    "\n",
    "\n",
    "final_labels_unlabeled = pd.Series(initial_labels.values + modification_values, name=TARGET).astype('float32')\n",
    "pd.DataFrame(final_labels_unlabeled).to_excel('outputs/20230315_adjusted_labels.xlsx', index=False)\n",
    "\n",
    "print(\"Stage-2 finished：Number of modified labels generated\", len(final_labels_unlabeled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd904c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_xtrain_data = pd.concat(\n",
    "    [normed_labeled_train_data.reset_index(drop=True),\n",
    "     normed_unlabeled_train_data.reset_index(drop=True)],\n",
    "    axis=0\n",
    ").astype('float32')\n",
    "\n",
    "\n",
    "final_ytrain_data = pd.concat(\n",
    "    [labeled_data_labels.reset_index(drop=True).astype('float32'),\n",
    "     final_labels_unlabeled.reset_index(drop=True).astype('float32')],\n",
    "    axis=0\n",
    ").squeeze()\n",
    "\n",
    "print(\"Final training data shape:\", final_xtrain_data.shape, final_ytrain_data.shape)\n",
    "\n",
    "final_model = build_model(final_xtrain_data.shape[1])\n",
    "\n",
    "stage3_callbacks = [\n",
    "    PrintDot(),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5, factor=0.5, min_lr=1e-6),\n",
    "]\n",
    "\n",
    "history = final_model.fit(\n",
    "    final_xtrain_data, final_ytrain_data,\n",
    "    epochs=EPOCHS_STAGE3, validation_split=0.2, verbose=0, callbacks=[stage3_callbacks]\n",
    ")\n",
    "\n",
    "hist3 = pd.DataFrame(history.history); hist3['epoch'] = history.epoch\n",
    "pd.DataFrame(hist3).to_excel('outputs/stage3_errors.xlsx', index=False)\n",
    "hist3.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_predictions = final_model.predict(normed_test_data, verbose=0).ravel()\n",
    "pd.DataFrame({\"y_pred\": test_predictions}).to_excel('outputs/20230406_DSSL_predictions.xlsx', index=False)\n",
    "pd.DataFrame(test_y_data).to_excel('outputs/20230406_test_actual.xlsx', index=False)\n",
    "pd.DataFrame(test_x_data).to_excel('outputs/20230406_test_X.xlsx', index=False)\n",
    "\n",
    "loss, mae, mse = final_model.evaluate(normed_test_data, test_y_data, verbose=0)\n",
    "print(\"loss:\", loss, \"MAE:\", mae, \"MSE:\", mse)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "print(\"R2 :\", r2_score(test_y_data, test_predictions))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(test_y_data, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5, factor=0.5, min_lr=1e-6),\n",
    "    ]\n",
    "\n",
    "\n",
    "X_train = final_xtrain_data.values.astype(\"float32\")\n",
    "y_train = final_ytrain_data.values.astype(\"float32\").ravel()\n",
    "X_test  = normed_test_data.values.astype(\"float32\")\n",
    "y_test  = test_y_data.values.astype(\"float32\").ravel()\n",
    "\n",
    "\n",
    "\n",
    "def build_model_tunable(input_dim,\n",
    "                        layers_: int = 3,\n",
    "                        units: int = 128,\n",
    "                        dropout: float = 0.2,\n",
    "                        l2: float = 1e-5,\n",
    "                        activation: str = \"relu\",\n",
    "                        learning_rate: float = 1e-3):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    kreg = regularizers.l2(l2) if l2 and l2 > 0 else None\n",
    "    for _ in range(layers_):\n",
    "        x = layers.Dense(units, kernel_regularizer=kreg)(x)\n",
    "        if activation == \"leaky_relu\":\n",
    "            x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            x = layers.Activation(activation)(x)\n",
    "        if dropout and dropout > 0:\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    m = keras.Model(inputs, outputs)\n",
    "    m.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss=\"mae\", metrics=[\"mae\"])\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b264840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "def sample_params():\n",
    "    return {\n",
    "        \"layers_\":       int(rng.choice([2, 3, 4])),\n",
    "        \"units\":         int(rng.choice([64, 128, 256])),\n",
    "        \"dropout\":       float(rng.uniform(0.0, 0.5)),\n",
    "        \"l2\":            float(10 ** rng.uniform(-6, -3)),\n",
    "        \"activation\":    str(rng.choice([\"relu\", \"leaky_relu\"])),\n",
    "        \"learning_rate\": float(10 ** rng.uniform(-4, math.log10(3e-2))),\n",
    "        \"batch_size\":    int(rng.choice([32, 64, 128])),\n",
    "    }\n",
    "\n",
    "\n",
    "def cv_mae(params, splits=CV_SPLITS):\n",
    "    kf = KFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "    maes = []\n",
    "    for tr_idx, va_idx in kf.split(X_train):\n",
    "        m = build_model_tunable(INPUT_DIM, **{k: v for k, v in params.items() if k != \"batch_size\"})\n",
    "        m.fit(\n",
    "            X_train[tr_idx], y_train[tr_idx],\n",
    "            epochs=EPOCHS_STAGE3, batch_size=params[\"batch_size\"],\n",
    "            validation_data=(X_train[va_idx], y_train[va_idx]),\n",
    "            callbacks=make_callbacks(), verbose=0\n",
    "        )\n",
    "        yv = m.predict(X_train[va_idx], verbose=0).ravel()\n",
    "        maes.append(mean_absolute_error(y_train[va_idx], yv))\n",
    "    return float(np.mean(maes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search（n = TUNING_TRIALS）\n",
    "trials = []\n",
    "for t in range(TUNING_TRIALS):\n",
    "    p = sample_params()\n",
    "    score = cv_mae(p, splits=CV_SPLITS)\n",
    "    trials.append({\"trial\": t+1, \"cv_mae\": score, **p})\n",
    "\n",
    "trials_df = pd.DataFrame(trials).sort_values(\"cv_mae\")\n",
    "best = trials_df.iloc[0].to_dict()\n",
    "best_params = {k: best[k] for k in best if k not in [\"trial\", \"cv_mae\"]}\n",
    "print(\"Best CV MAE:\", best[\"cv_mae\"])\n",
    "print(\"Best params:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuned_model = build_model_tunable(\n",
    "    INPUT_DIM, **{k: v for k, v in best_params.items() if k != \"batch_size\"}\n",
    ")\n",
    "\n",
    "hist_tuned = tuned_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS_STAGE3,\n",
    "    batch_size=int(best_params[\"batch_size\"]),\n",
    "    validation_split=0.2,\n",
    "    callbacks=make_callbacks(),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "hist_tuned_df = pd.DataFrame(hist_tuned.history)\n",
    "\n",
    "\n",
    "hist_tuned_df[\"epoch\"] = np.arange(1, len(hist_tuned_df[\"loss\"]) + 1)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "hist_tuned_df.to_excel(\"outputs/stage3_tuned_errors.xlsx\", index=False)\n",
    "\n",
    "\n",
    "best_epoch = int(np.argmin(hist_tuned.history[\"val_loss\"])) + 1\n",
    "best_val   = float(np.min(hist_tuned.history[\"val_loss\"]))\n",
    "print(f\"[tuned] best_epoch={best_epoch}, best_val_MAE={best_val:.6f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "tuned_pred = tuned_model.predict(X_test, verbose=0).ravel()\n",
    "tuned_test_mae = mean_absolute_error(y_test, tuned_pred)\n",
    "print(\"Tuned Test MAE:\", tuned_test_mae)\n",
    "\n",
    "\n",
    "tuned_model.save(\"outputs/stage3_tuned_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64712735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "trials_df.to_excel(\"outputs/stage3_randomsearch_trials.xlsx\", index=False)\n",
    "pd.DataFrame([{\n",
    "    \"Method\": \"RandomSearch (Stage-3)\",\n",
    "    \"CV MAE (best)\": best[\"cv_mae\"],\n",
    "    \"Test MAE\": tuned_test_mae,\n",
    "    \"Best Params\": best_params\n",
    "}]).to_excel(\"outputs/stage3_tuning_summary.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def quick_eval(model, X, y):\n",
    "    y_pred = model.predict(X, verbose=0).ravel()\n",
    "    mae  = mean_absolute_error(y, y_pred)\n",
    "    mse  = mean_squared_error(y, y_pred)\n",
    "    r2   = r2_score(y, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y, y_pred)\n",
    "    return mae, mse, r2, mape\n",
    "\n",
    "print(\"Baseline (final_model):\", *quick_eval(final_model, normed_test_data, np.asarray(test_y_data).ravel()))\n",
    "if \"tuned_model\" in globals():\n",
    "    print(\"Tuned:\", *quick_eval(tuned_model, normed_test_data, np.asarray(test_y_data).ravel()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
