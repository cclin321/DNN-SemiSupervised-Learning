{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pandas numpy scikit-learn matplotlib openpyxl scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3add1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, tensorflow as tf\n",
    "print(sys.executable) \n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "\n",
    "random.seed(42); np.random.seed(42); tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"Grid  Power\"  \n",
    "FILTER_Y_EQ_ZERO = True   \n",
    "\n",
    "\n",
    "raw_data = pd.read_excel('raw data.xlsx')\n",
    "\n",
    "\n",
    "raw_data = raw_data.drop(columns=['times'], errors='ignore').copy()\n",
    "\n",
    "\n",
    "if FILTER_Y_EQ_ZERO:\n",
    "    raw_data = raw_data[raw_data[TARGET] != 0].copy()\n",
    "\n",
    "\n",
    "num_cols = [c for c in raw_data.columns if c != TARGET]\n",
    "raw_data[num_cols] = raw_data[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "raw_data = raw_data.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Data shape:\", raw_data.shape)\n",
    "display(raw_data.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train/Test (80/20)\n",
    "train_dataset = raw_data.sample(frac=0.8, random_state=0)\n",
    "test_dataset  = raw_data.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "test_x_data = test_dataset.drop(columns=[TARGET]).copy()\n",
    "test_y_data = test_dataset[TARGET].copy()\n",
    "\n",
    "\n",
    "labeled_train_data   = train_dataset.sample(frac=0.6, random_state=0).copy()\n",
    "unlabeled_train_data = train_dataset.drop(labeled_train_data.index).copy()\n",
    "\n",
    "\n",
    "unlabeled_train_data_actual = unlabeled_train_data.pop(TARGET).copy()\n",
    "labeled_data_labels         = labeled_train_data.pop(TARGET).copy()\n",
    "\n",
    "len(train_dataset), len(labeled_train_data), len(unlabeled_train_data), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "print(\"Columns:\", train_dataset.columns.tolist())\n",
    "\n",
    "\n",
    "feature_cols = [c for c in train_dataset.columns if c != TARGET]\n",
    "\n",
    "\n",
    "scaler_fs = StandardScaler().fit(labeled_train_data[feature_cols])\n",
    "\n",
    "\n",
    "X_fs_tr = scaler_fs.transform(labeled_train_data[feature_cols]).astype(\"float32\")\n",
    "y_fs_tr = labeled_data_labels.values.astype(\"float32\")  \n",
    "\n",
    "X_fs_te = scaler_fs.transform(test_dataset[feature_cols]).astype(\"float32\")\n",
    "y_fs_te = test_dataset[TARGET].values.astype(\"float32\")\n",
    "\n",
    "INPUT_DIM_FS = X_fs_tr.shape[1]\n",
    "print(\"Supervised shapes → X_tr:\", X_fs_tr.shape, \" X_te:\", X_fs_te.shape, \"  INPUT_DIM_FS:\", INPUT_DIM_FS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71482cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Mean-Teacher (Π-model, regression) — data prep ===\n",
    "\n",
    "\n",
    "X_u = scaler_fs.transform(unlabeled_train_data[feature_cols]).astype(\"float32\")\n",
    "\n",
    "\n",
    "n_all = X_fs_tr.shape[0]\n",
    "n_val = int(round(n_all * 0.20))\n",
    "X_l_tr, y_l_tr = X_fs_tr[:n_all - n_val], y_fs_tr[:n_all - n_val]\n",
    "X_l_va, y_l_va = X_fs_tr[n_all - n_val:], y_fs_tr[n_all - n_val:]\n",
    "\n",
    "X_u.shape, X_l_tr.shape, X_l_va.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(\n",
    "    input_dim: int,\n",
    "    layers_: int = 3,\n",
    "    units: int = 32,\n",
    "    dropout: float = 0.0,\n",
    "    l2: float = 0.0,\n",
    "    activation: str = \"relu\",      \n",
    "    learning_rate: float = 1e-3,\n",
    "    loss: str = \"mae\",\n",
    "    metrics=(\"mae\",\"mse\"),\n",
    "):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    kreg = regularizers.l2(l2) if (l2 and l2 > 0) else None\n",
    "    for _ in range(layers_):\n",
    "        x = layers.Dense(units, kernel_regularizer=kreg)(x)\n",
    "        if activation == \"leaky_relu\":\n",
    "            x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            x = layers.Activation(activation)(x)\n",
    "        if dropout and dropout > 0:\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=list(metrics),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19130618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Mean-Teacher (Π-model, regression) — train & evaluate & save ===\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "EPOCHS         = 120\n",
    "BATCH_L        = 64\n",
    "BATCH_U        = 64\n",
    "LR             = 3e-4     \n",
    "EMA_M          = 0.999    \n",
    "LAMBDA_U_MAX   = 0.2      \n",
    "RAMP_UP_EPO    = 30        \n",
    "BURN_IN_EPO    = 8         \n",
    "NOISE_STD      = 0.02      \n",
    "CLIP_NORM      = 1.0      \n",
    "\n",
    "\n",
    "X_l_tr = X_l_tr.astype(\"float32\"); y_l_tr = y_l_tr.astype(\"float32\")\n",
    "X_l_va = X_l_va.astype(\"float32\"); y_l_va = y_l_va.astype(\"float32\")\n",
    "X_fs_te = X_fs_te.astype(\"float32\"); y_fs_te = y_fs_te.astype(\"float32\")\n",
    "X_u = X_u.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3241b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "student = build_model(INPUT_DIM_FS, layers_=3, units=32, dropout=0.0, l2=0.0,\n",
    "                      activation=\"relu\", learning_rate=LR, loss=\"mae\")\n",
    "pre_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=10, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\", factor=0.5, patience=5, min_lr=1e-5),\n",
    "]\n",
    "student.fit(\n",
    "    X_l_tr, y_l_tr,\n",
    "    validation_data=(X_l_va, y_l_va),\n",
    "    epochs=40, batch_size=BATCH_L, verbose=0,\n",
    "    callbacks=pre_callbacks\n",
    ")\n",
    "\n",
    "\n",
    "teacher = build_model(INPUT_DIM_FS, layers_=3, units=32, dropout=0.0, l2=0.0,\n",
    "                      activation=\"relu\", learning_rate=LR, loss=\"mae\")\n",
    "teacher.set_weights(student.get_weights())\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LR, clipnorm=CLIP_NORM)\n",
    "\n",
    "def ds_labeled(X, y, bs): return tf.data.Dataset.from_tensor_slices((X, y)).shuffle(2048).repeat().batch(bs)\n",
    "def ds_unlabeled(X, bs):  return tf.data.Dataset.from_tensor_slices(X).shuffle(2048).repeat().batch(bs)\n",
    "dsL = iter(ds_labeled(X_l_tr, y_l_tr, BATCH_L))\n",
    "dsU = iter(ds_unlabeled(X_u, BATCH_U))\n",
    "steps_per_epoch = max(len(X_l_tr) // BATCH_L, 1)\n",
    "\n",
    "@tf.function\n",
    "def add_noise(x, std=NOISE_STD):\n",
    "    return x + tf.random.normal(tf.shape(x), stddev=std)\n",
    "\n",
    "train_hist = []\n",
    "best_val_mae = float(\"inf\")\n",
    "best_teacher_weights = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ramp-up & burn-in\n",
    "    if epoch <= BURN_IN_EPO:\n",
    "        lambda_u = 0.0\n",
    "    else:\n",
    "        ramp = tf.clip_by_value((epoch - BURN_IN_EPO) / max(RAMP_UP_EPO, 1), 0.0, 1.0)\n",
    "        lambda_u = float(LAMBDA_U_MAX * ramp.numpy())\n",
    "\n",
    "    sup_mae_acc = keras.metrics.Mean()\n",
    "    cons_mse_acc = keras.metrics.Mean()\n",
    "    total_acc = keras.metrics.Mean()\n",
    "\n",
    "    for step in range(steps_per_epoch):\n",
    "        xb_l, yb_l = next(dsL)\n",
    "        xb_u = next(dsU)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            pred_l = tf.squeeze(student(add_noise(xb_l, NOISE_STD), training=True), axis=-1)\n",
    "            sup_loss = tf.reduce_mean(tf.abs(yb_l - pred_l))  # L1/MAE\n",
    "\n",
    "            u1 = add_noise(xb_u, NOISE_STD)\n",
    "            u2 = add_noise(xb_u, NOISE_STD)\n",
    "            stu_u = tf.squeeze(student(u1, training=True), axis=-1)\n",
    "            tea_u = tf.squeeze(teacher(u2, training=False), axis=-1)\n",
    "            cons_loss = tf.reduce_mean(tf.square(stu_u - tf.stop_gradient(tea_u)))  # L2/MSE\n",
    "\n",
    "            loss = sup_loss + lambda_u * cons_loss\n",
    "\n",
    "        grads = tape.gradient(loss, student.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, student.trainable_variables))\n",
    "\n",
    "     \n",
    "        tw, sw = teacher.get_weights(), student.get_weights()\n",
    "        teacher.set_weights([EMA_M * t + (1.0 - EMA_M) * s for t, s in zip(tw, sw)])\n",
    "\n",
    "        sup_mae_acc.update_state(sup_loss)\n",
    "        cons_mse_acc.update_state(cons_loss)\n",
    "        total_acc.update_state(loss)\n",
    "\n",
    "  \n",
    "    val_pred = teacher.predict(X_l_va, verbose=0).ravel()\n",
    "    val_mae = float(np.mean(np.abs(val_pred - y_l_va)))\n",
    "\n",
    "    train_hist.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"sup_mae\": float(sup_mae_acc.result().numpy()),\n",
    "        \"cons_mse\": float(cons_mse_acc.result().numpy()),\n",
    "        \"total\": float(total_acc.result().numpy()),\n",
    "        \"val_mae\": val_mae,\n",
    "        \"lambda_u\": lambda_u,\n",
    "    })\n",
    "\n",
    "  \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_teacher_weights = teacher.get_weights()\n",
    "\n",
    "\n",
    "if best_teacher_weights is not None:\n",
    "    teacher.set_weights(best_teacher_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics_block(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    mse  = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return {\"Loss\": mae, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape}\n",
    "\n",
    "yhat_tr = teacher.predict(X_l_tr, verbose=0).ravel()\n",
    "yhat_va = teacher.predict(X_l_va, verbose=0).ravel()\n",
    "yhat_te = teacher.predict(X_fs_te, verbose=0).ravel()\n",
    "\n",
    "m_tr = metrics_block(y_l_tr, yhat_tr)\n",
    "m_va = metrics_block(y_l_va, yhat_va)\n",
    "m_te = metrics_block(y_fs_te, yhat_te)\n",
    "\n",
    "hist_df = pd.DataFrame(train_hist)\n",
    "pred_train_df = pd.DataFrame({\"y_true\": y_l_tr, \"y_pred\": yhat_tr, \"residual\": y_l_tr - yhat_tr})\n",
    "pred_val_df   = pd.DataFrame({\"y_true\": y_l_va, \"y_pred\": yhat_va, \"residual\": y_l_va - yhat_va})\n",
    "pred_test_df  = pd.DataFrame({\"y_true\": y_fs_te, \"y_pred\": yhat_te, \"residual\": y_fs_te - yhat_te})\n",
    "summary_df = pd.DataFrame.from_dict({\"Train\": m_tr, \"Val\": m_va, \"Test\": m_te}, orient=\"index\")[[\"Loss\",\"MAE\",\"MSE\",\"RMSE\",\"R2\",\"MAPE\"]]\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "pd.DataFrame({\"y_true\": y_fs_te, \"y_pred\": yhat_te}).to_excel(\"outputs/mean_teacher_test_preds.xlsx\", index=False)\n",
    "with pd.ExcelWriter(\"outputs/mean_teacher_errors.xlsx\", engine=\"openpyxl\") as w:\n",
    "    hist_df.to_excel(w, sheet_name=\"history\", index=False)\n",
    "    summary_df.to_excel(w, sheet_name=\"summary\")\n",
    "    pred_train_df.to_excel(w, sheet_name=\"pred_train\", index=False)\n",
    "    pred_val_df.to_excel(w,   sheet_name=\"pred_val\",   index=False)\n",
    "    pred_test_df.to_excel(w,  sheet_name=\"pred_test\",  index=False)\n",
    "\n",
    "print(\"[Mean-Teacher] Summary:\\n\", summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
