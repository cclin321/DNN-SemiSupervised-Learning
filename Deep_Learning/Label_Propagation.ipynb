{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pandas numpy scikit-learn matplotlib openpyxl scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3add1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, tensorflow as tf\n",
    "print(sys.executable) \n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "\n",
    "random.seed(42); np.random.seed(42); tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"Grid  Power\"  \n",
    "FILTER_Y_EQ_ZERO = True   \n",
    "\n",
    "\n",
    "raw_data = pd.read_excel('raw data.xlsx')\n",
    "\n",
    "\n",
    "raw_data = raw_data.drop(columns=['times'], errors='ignore').copy()\n",
    "\n",
    "\n",
    "if FILTER_Y_EQ_ZERO:\n",
    "    raw_data = raw_data[raw_data[TARGET] != 0].copy()\n",
    "\n",
    "\n",
    "num_cols = [c for c in raw_data.columns if c != TARGET]\n",
    "raw_data[num_cols] = raw_data[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "raw_data = raw_data.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Data shape:\", raw_data.shape)\n",
    "display(raw_data.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = raw_data.sample(frac=0.8, random_state=0)\n",
    "test_dataset  = raw_data.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "test_x_data = test_dataset.drop(columns=[TARGET]).copy()\n",
    "test_y_data = test_dataset[TARGET].copy()\n",
    "\n",
    "\n",
    "labeled_train_data   = train_dataset.sample(frac=0.6, random_state=0).copy()\n",
    "unlabeled_train_data = train_dataset.drop(labeled_train_data.index).copy()\n",
    "\n",
    "\n",
    "unlabeled_train_data_actual = unlabeled_train_data.pop(TARGET).copy()\n",
    "labeled_data_labels         = labeled_train_data.pop(TARGET).copy()\n",
    "\n",
    "len(train_dataset), len(labeled_train_data), len(unlabeled_train_data), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "print(\"Columns:\", train_dataset.columns.tolist())\n",
    "\n",
    "\n",
    "feature_cols = [c for c in train_dataset.columns if c != TARGET]\n",
    "\n",
    "\n",
    "scaler_fs = StandardScaler().fit(labeled_train_data[feature_cols])\n",
    "\n",
    "\n",
    "X_fs_tr = scaler_fs.transform(labeled_train_data[feature_cols]).astype(\"float32\")\n",
    "y_fs_tr = labeled_data_labels.values.astype(\"float32\")   \n",
    "\n",
    "\n",
    "X_fs_te = scaler_fs.transform(test_dataset[feature_cols]).astype(\"float32\")\n",
    "y_fs_te = test_dataset[TARGET].values.astype(\"float32\")\n",
    "\n",
    "INPUT_DIM_FS = X_fs_tr.shape[1]\n",
    "print(\"Supervised shapes → X_tr:\", X_fs_tr.shape, \" X_te:\", X_fs_te.shape, \"  INPUT_DIM_FS:\", INPUT_DIM_FS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === data prep ===\n",
    "\n",
    "X_u = scaler_fs.transform(unlabeled_train_data[feature_cols]).astype(\"float32\")\n",
    "\n",
    "\n",
    "n_all = X_fs_tr.shape[0]\n",
    "n_val = int(round(n_all * 0.20))\n",
    "X_l_tr, y_l_tr = X_fs_tr[:n_all - n_val], y_fs_tr[:n_all - n_val]\n",
    "X_l_va, y_l_va = X_fs_tr[n_all - n_val:], y_fs_tr[n_all - n_val:]\n",
    "\n",
    "X_u.shape, X_l_tr.shape, X_l_va.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Label Propagation (continuous) — build graph & propagate ===\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix, diags, identity\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "\n",
    "K_NEIGHBORS = 15     \n",
    "ALPHA       = 0.9    \n",
    "SIGMA_MODE  = \"median\"  \n",
    "\n",
    "\n",
    "X_all = np.vstack([X_l_tr, X_u])\n",
    "n_l   = X_l_tr.shape[0]\n",
    "n_u   = X_u.shape[0]\n",
    "n     = n_l + n_u\n",
    "assert n_l > 0 and n_u > 0, \"labeled or unlabeled is empty，Please check the split ratio.。\"\n",
    "\n",
    "\n",
    "nbr_tmp = NearestNeighbors(n_neighbors=min(10, max(2, X_all.shape[0]-1)), metric='euclidean').fit(X_all)\n",
    "dists_tmp, _ = nbr_tmp.kneighbors(X_all)\n",
    "if SIGMA_MODE == \"median\":\n",
    "    sigma = np.median(dists_tmp[:, 1:]) + 1e-12  \n",
    "else:\n",
    "    sigma = float(SIGMA_MODE)\n",
    "print(f\"[LP] RBF sigma = {sigma:.6f}\")\n",
    "\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=min(K_NEIGHBORS, n-1), metric='euclidean').fit(X_all)\n",
    "dists, inds = nbrs.kneighbors(X_all)  # [n, k]\n",
    "rows = np.repeat(np.arange(n), inds.shape[1])\n",
    "cols = inds.ravel()\n",
    "w    = np.exp(-(dists**2) / (2.0 * sigma**2)).ravel()\n",
    "W    = csr_matrix((w, (rows, cols)), shape=(n, n))\n",
    "W    = W.maximum(W.T)  \n",
    "\n",
    "\n",
    "d = np.asarray(W.sum(axis=1)).ravel() + 1e-12\n",
    "D_inv = diags(1.0 / d)\n",
    "S = (D_inv @ W).tocsr()\n",
    "\n",
    "\n",
    "S_ul = S[n_l:n, 0:n_l]\n",
    "S_uu = S[n_l:n, n_l:n]\n",
    "Iuu  = identity(n_u, format=\"csc\")\n",
    "rhs  = (ALPHA * S_ul) @ y_l_tr\n",
    "A    = Iuu - (ALPHA * S_uu).tocsc()\n",
    "\n",
    "\n",
    "y_u_hat = spsolve(A, rhs)\n",
    "\n",
    "print(f\"[LP] propagated unlabeled = {y_u_hat.shape}, mean={float(y_u_hat.mean()):.4f}, std={float(y_u_hat.std()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inductive prediction (RBF smoother, no test leakage) + export ===\n",
    "import os, pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "\n",
    "def rbf_knn_predict(X_train, y_train, X_query, k=15, sigma=1.0):\n",
    "    nbr = NearestNeighbors(n_neighbors=min(k, max(1, X_train.shape[0])), metric='euclidean').fit(X_train)\n",
    "    d, ind = nbr.kneighbors(X_query)  # d: [m,k], ind: [m,k]\n",
    "    w = np.exp(-(d**2) / (2.0 * sigma**2)) + 1e-12\n",
    "    y_neighbors = y_train[ind]        # [m,k]\n",
    "    return (w * y_neighbors).sum(axis=1) / w.sum(axis=1)\n",
    "\n",
    "\n",
    "X_tr_aug = np.vstack([X_l_tr, X_u])\n",
    "y_tr_aug = np.concatenate([y_l_tr, y_u_hat])\n",
    "\n",
    "\n",
    "yhat_tr = rbf_knn_predict(X_tr_aug, y_tr_aug, X_l_tr, k=K_NEIGHBORS, sigma=sigma)\n",
    "yhat_va = rbf_knn_predict(X_tr_aug, y_tr_aug, X_l_va, k=K_NEIGHBORS, sigma=sigma)\n",
    "yhat_te = rbf_knn_predict(X_tr_aug, y_tr_aug, X_fs_te, k=K_NEIGHBORS, sigma=sigma)\n",
    "\n",
    "def mb(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    mse  = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return {\"Loss\": mae, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape}\n",
    "\n",
    "m_tr = mb(y_l_tr, yhat_tr)\n",
    "m_va = mb(y_l_va, yhat_va)\n",
    "m_te = mb(y_fs_te, yhat_te)\n",
    "\n",
    "summary_df = pd.DataFrame.from_dict({\"Train\": m_tr, \"Val\": m_va, \"Test\": m_te},\n",
    "                                    orient=\"index\")[[\"Loss\",\"MAE\",\"MSE\",\"RMSE\",\"R2\",\"MAPE\"]]\n",
    "print(\"[Label Propagation (continuous)] Summary:\\n\", summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "pd.DataFrame({\"y_true\": y_fs_te, \"y_pred\": yhat_te}).to_excel(\"outputs/labelprop_test_preds.xlsx\", index=False)\n",
    "\n",
    "with pd.ExcelWriter(\"outputs/labelprop_errors.xlsx\", engine=\"openpyxl\") as w:\n",
    "    summary_df.to_excel(w, sheet_name=\"summary\")\n",
    "    pd.DataFrame({\"y_true\": y_l_tr, \"y_pred\": yhat_tr, \"residual\": y_l_tr - yhat_tr}).to_excel(w, sheet_name=\"pred_train\", index=False)\n",
    "    pd.DataFrame({\"y_true\": y_l_va, \"y_pred\": yhat_va, \"residual\": y_l_va - yhat_va}).to_excel(w, sheet_name=\"pred_val\", index=False)\n",
    "    pd.DataFrame({\"y_true\": y_fs_te, \"y_pred\": yhat_te, \"residual\": y_fs_te - yhat_te}).to_excel(w, sheet_name=\"pred_test\", index=False)\n",
    "    pd.DataFrame({\"lp_pseudo_y_unlabeled\": y_u_hat}).to_excel(w, sheet_name=\"lp_pseudo_unlabeled\", index=False)\n",
    "\n",
    "print(\"Saved → outputs/labelprop_errors.xlsx, outputs/labelprop_test_preds.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
